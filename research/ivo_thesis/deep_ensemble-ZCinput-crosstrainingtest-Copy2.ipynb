{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep ensemble for ENSO-forecasting\n",
    "\n",
    "In this tutorial you learn how to use a neural network model called Deep Ensemble (DE) for the ENSO forecasting. This network architecture was initially developed [Lakshminarayanan et al. (2017)](https://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf). \n",
    "\n",
    "DEs are feed foreword neural networks that predict the mean and the standard deviation of a Gaussian. Hence, their predicion comes with an uncertainty estimation which is a valuable feature for ENSO-forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data pipe line\n",
    "\n",
    "At first, we define a data pipeline. This is in general quite useful to keep your code clean and also to reuse the pipeline for later purpose.\n",
    "\n",
    "The data pipeline generates returns:\n",
    "\n",
    "1. The feature array\n",
    "\n",
    "2. The label array\n",
    "\n",
    "3. The time  array corresponding to the time of the label\n",
    "\n",
    "NOTE (again): Lead time is defined as the time that passed between the last observed and the first date of the target season. Hence, negative appear, e.g. if you compare the DJF season with the target season JFM, you have a lead time of -2 month (Last observed date: Feburary 28/29, First date of the target season January 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ninolearn.IO.read_processed import data_reader\n",
    "from ninolearn.IO.read_raw import ZC_simple_read\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ninolearn.learn.fit import n_decades, lead_times, decade_color, decade_name\n",
    "from ninolearn.learn.evaluation import evaluation_correlation, evaluation_decadal_correlation, evaluation_seasonal_correlation, evaluation_decadal_correlation_ZC\n",
    "from ninolearn.learn.fit import cross_hindcast_dem\n",
    "from ninolearn.plot.evaluation import plot_seasonal_skill_ZC\n",
    "import matplotlib.pyplot as plt\n",
    "from ninolearn.learn.fit import cross_training\n",
    "from ninolearn.learn.fit import cross_hindcast_dem\n",
    "from ninolearn.learn.models import DEM\n",
    "\n",
    "oneyear= pd.Timedelta(365, 'D')\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tstart = 1952-04-07 14:17:30 and tend = 1994-08-26 03:33:20 (train)\n",
      "tstart = 1952-04-07 14:17:30 and tend = 1994-08-26 03:33:20 (test)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "train_version = 'de12lr'\n",
    "test_version = 'mu28v4'\n",
    "name = 'dem' + '_'+ train_version  + '_' + test_version\n",
    "# leadtime = 12\n",
    "\n",
    "# t_start is defined using a funky timedelta because the starting date of the network analysis data is the last month\n",
    "# of its start year which is 1951-12 therefore the time must start in 1952 with some months added for values lost in \n",
    "# interpolation. TODO: fix this by backwards interpolating the first values of the year and finding out what is happening \n",
    "# with the nms\n",
    "train_times = np.unique(ZC_simple_read(train_version)['time'])\n",
    "test_times = np.unique(ZC_simple_read(test_version)['time'])\n",
    "\n",
    "train_t_start = train_times[0] + pd.Timedelta((2*365 + 90),'D')\n",
    "train_t_end = train_times[-1] - pd.Timedelta(90,'D')\n",
    "\n",
    "test_t_start = test_times[0] + pd.Timedelta((2*365 + 90),'D')\n",
    "test_t_end = test_times[-1] - pd.Timedelta(90,'D')\n",
    "\n",
    "print(f'tstart = {train_t_start} and tend = {train_t_end} (train)')\n",
    "print(f'tstart = {test_t_start} and tend = {test_t_end} (test)')\n",
    "\n",
    "t_start = train_t_start\n",
    "t_end = train_t_end\n",
    "times = train_times\n",
    "\n",
    "if train_t_end < pd.Timestamp('1990-01-01') or test_t_end < pd.Timestamp('1990-01-01'):\n",
    "    raise ValueError('one or both timeseries are too short!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "time = pd.Timestamp('1999-01-01')\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'lat': length 31 on 'lat' and length 21 on 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e667d09b4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## read raw ZC data and save to 1x1 grid file in processeddir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## also makes field of h and sst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mZC_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## calculates monthly averaged (?) fields of thermocline height within region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ninolearn/ninolearn/IO/read_raw.py\u001b[0m in \u001b[0;36mZC_raw\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m    325\u001b[0m         coords = { \"lat\": (['lat'], lats_ZC),\n\u001b[1;32m    326\u001b[0m                   \u001b[0;34m\"lon\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlons_ZC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                   \"time\": tvals},)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n\u001b[0;32m--> 544\u001b[0;31m             \u001b[0mdata_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"broadcast_equals\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         )\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data, coords, compat, join)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_indexes_from_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     return merge_core(\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0massert_unique_multiindex_level_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0mcoord_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoncoord_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermine_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mcalculate_dimensions\u001b[0;34m(variables)\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;34m\"conflicting sizes for dimension %r: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0;34m\"length %s on %r and length %s on %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                     \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_used\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 )\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'lat': length 31 on 'lat' and length 21 on 'temperature'"
     ]
    }
   ],
   "source": [
    "from ninolearn.IO.read_raw import ZC_raw, ZC_h, ZC_oni\n",
    "from ninolearn.preprocess.prepare import prep_nms\n",
    "from ninolearn.plot.ZC_dem_plots import nms_plots\n",
    "## read raw ZC data and save to 1x1 grid file in processeddir\n",
    "## also makes field of h and sst\n",
    "ZC_raw(train_version)\n",
    "\n",
    "## calculates monthly averaged (?) fields of thermocline height within region \n",
    "## of interest. cacluate ONI in region of interest. calculate network metrics \n",
    "## from sst (Henk's suggestion) or thermocline height (like Paul)\n",
    "ZC_h(train_version)\n",
    "ZC_oni(train_version)\n",
    "\n",
    "prep_nms(train_version, 0.99, t_start, t_end)\n",
    "\n",
    "# make plots\n",
    "# nms_plots(train_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZC_raw(test_version)\n",
    "ZC_h(test_version)\n",
    "ZC_oni(test_version)\n",
    "prep_nms(test_version, 0.99, t_start, t_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = data_reader(startdate=(t_start + pd.Timedelta(365,'D')) , enddate=(t_end - pd.Timedelta(2*365, 'D')) , lon_min = 124, lon_max = 280,\n",
    "                         lat_min = -19, lat_max = 19)\n",
    "\n",
    "oni = reader.read_csv(('oni_ZC_' +train_version))\n",
    "h = reader.read_csv(('h_mean_ZC_' + train_version))\n",
    "\n",
    "network_ssh = reader.read_statistic('network_metrics', variable='sst', dataset=('ZC_25x25_' + train_version), processed=\"anom\")\n",
    "c2 = network_ssh['fraction_clusters_size_2']\n",
    "H = network_ssh['corrected_hamming_distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if h.shape[0] + c2.shape[0] - H.shape[0] - oni.shape[0] == 0:\n",
    "    print('All datasets are of equal length')\n",
    "else:\n",
    "    print('warning: datasets not of equal size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ninolearn.utils import include_time_lag\n",
    "from ninolearn.IO.read_processed import data_reader\n",
    "\n",
    "version = train_version\n",
    "\n",
    "def train_pipeline(lead_time):\n",
    "    version = train_version\n",
    "    \"\"\"\n",
    "    Data pipeline for the processing of the data before the Deep Ensemble\n",
    "    is trained.\n",
    "\n",
    "    :type lead_time: int\n",
    "    :param lead_time: The lead time in month.\n",
    "\n",
    "    :returns: The feature \"X\" (at observation time), the label \"y\" (at lead\n",
    "    time), the target season \"timey\" (least month)\n",
    "    \"\"\"\n",
    "    timelag=False\n",
    "#     reader = data_reader(startdate='1952-01', enddate='1992-12', lon_min = 124, lon_max = 280,\n",
    "#                          lat_min = -19, lat_max = 19)\n",
    "    reader = data_reader(startdate=(t_start + oneyear), enddate=(t_end - 2*oneyear), lon_min = 124, lon_max = 280,\n",
    "                         lat_min = -19, lat_max = 19)\n",
    "\n",
    "    # indeces\n",
    "    oni = reader.read_csv(('oni_ZC_' +version))\n",
    "    h = reader.read_csv(('h_mean_ZC_' + version))\n",
    "    #IOD unavailable in ZC87 model \n",
    "    \n",
    "    # seasonal cycle\n",
    "    sc = np.cos(np.arange(len(oni))/12*2*np.pi)\n",
    "\n",
    "    # network metrics\n",
    "    network_ssh = reader.read_statistic('network_metrics', variable='sst', dataset=('ZC_25x25_'+version), processed=\"anom\")\n",
    "    c2 = network_ssh['fraction_clusters_size_2']\n",
    "    H = network_ssh['corrected_hamming_distance']\n",
    "\n",
    "    # time lag\n",
    "    time_lag = 12\n",
    "\n",
    "    # shift such that lead time corresponds to the definition of lead time\n",
    "    shift = 3\n",
    "\n",
    "    # process features\n",
    "    feature_unscaled = np.stack((oni, h,\n",
    "                                 c2, H), axis=1)\n",
    "\n",
    "    # scale each feature\n",
    "    scalerX = StandardScaler()\n",
    "    Xorg = scalerX.fit_transform(feature_unscaled)\n",
    "\n",
    "    # set nans to 0.\n",
    "    Xorg = np.nan_to_num(Xorg)\n",
    "\n",
    "    # arange the feature array\n",
    "    X = Xorg[:-lead_time-shift,:] # this chops of a bit at the end because matching labels will be offset by \n",
    "    # this amount. e.g. if our data runs until 2012 we need to remove X values for 2012 because we will use december 2011\n",
    "    # to predict december 2012 \n",
    "    \n",
    "#     X = include_time_lag(X, max_lag=time_lag)\n",
    "    X = include_time_lag(X, n_lags =time_lag)  # staggers the data with 1 month shifts so at each moment of input also\n",
    "    # nlags amount of months before is available to the AI\n",
    "        \n",
    "    # arange label\n",
    "    yorg = oni.values\n",
    "    y = yorg[lead_time + time_lag + shift:] # labels offset by lead_time to predict into the future and time_lag \n",
    "    # because the include_time_lag function shifts X values forward by an amount n_lags=time_lag\n",
    "    \n",
    "    # get the time axis of the label\n",
    "    timey = oni.index[lead_time + time_lag + shift:]\n",
    "\n",
    "    if timelag == False:\n",
    "        X = Xorg\n",
    "        y = yorg\n",
    "        timey = oni.index\n",
    "        \n",
    "    return X, y, timey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = test_version\n",
    "\n",
    "def test_pipeline(lead_time):\n",
    "    version = test_version\n",
    "\n",
    "    \"\"\"\n",
    "    Data pipeline for the processing of the data before the Deep Ensemble\n",
    "    is trained.\n",
    "\n",
    "    :type lead_time: int\n",
    "    :param lead_time: The lead time in month.\n",
    "\n",
    "    :returns: The feature \"X\" (at observation time), the label \"y\" (at lead\n",
    "    time), the target season \"timey\" (least month)\n",
    "    \"\"\"\n",
    "    timelag=False\n",
    "#     reader = data_reader(startdate='1952-01', enddate='1992-12', lon_min = 124, lon_max = 280,\n",
    "#                          lat_min = -19, lat_max = 19)\n",
    "    reader = data_reader(startdate=(t_start + oneyear), enddate=(t_end - 2*oneyear), lon_min = 124, lon_max = 280,\n",
    "                         lat_min = -19, lat_max = 19)\n",
    "\n",
    "    # indeces\n",
    "    oni = reader.read_csv(('oni_ZC_' +version))\n",
    "    h = reader.read_csv(('h_mean_ZC_' + version))\n",
    "    #IOD unavailable in ZC87 model \n",
    "    \n",
    "    # seasonal cycle\n",
    "    sc = np.cos(np.arange(len(oni))/12*2*np.pi)\n",
    "\n",
    "    # network metrics\n",
    "    network_ssh = reader.read_statistic('network_metrics', variable='sst', dataset=('ZC_25x25_'+version), processed=\"anom\")\n",
    "    c2 = network_ssh['fraction_clusters_size_2']\n",
    "    H = network_ssh['corrected_hamming_distance']\n",
    "\n",
    "    # time lag\n",
    "    time_lag = 12\n",
    "\n",
    "    # shift such that lead time corresponds to the definition of lead time\n",
    "    shift = 3\n",
    "\n",
    "    # process features\n",
    "    feature_unscaled = np.stack((oni, h,\n",
    "                                 c2, H), axis=1)\n",
    "\n",
    "    # scale each feature\n",
    "    scalerX = StandardScaler()\n",
    "    Xorg = scalerX.fit_transform(feature_unscaled)\n",
    "\n",
    "    # set nans to 0.\n",
    "    Xorg = np.nan_to_num(Xorg)\n",
    "\n",
    "    # arange the feature array\n",
    "    X = Xorg[:-lead_time-shift,:] # this chops of a bit at the end because matching labels will be offset by \n",
    "    # this amount. e.g. if our data runs until 2012 we need to remove X values for 2012 because we will use december 2011\n",
    "    # to predict december 2012 \n",
    "    \n",
    "#     X = include_time_lag(X, max_lag=time_lag)\n",
    "    X = include_time_lag(X, n_lags =time_lag)  # staggers the data with 1 month shifts so at each moment of input also\n",
    "    # nlags amount of months before is available to the AI\n",
    "        \n",
    "    # arange label\n",
    "    yorg = oni.values\n",
    "    y = yorg[lead_time + time_lag + shift:] # labels offset by lead_time to predict into the future and time_lag \n",
    "    # because the include_time_lag function shifts X values forward by an amount n_lags=time_lag\n",
    "    \n",
    "    # get the time axis of the label\n",
    "    timey = oni.index[lead_time + time_lag + shift:]\n",
    "\n",
    "    if timelag == False:\n",
    "        X = Xorg\n",
    "        y = yorg\n",
    "        timey = oni.index\n",
    "        \n",
    "    return X, y, timey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _, _, x3, _, _= *train_pipeline(0), *train_pipeline(3)\n",
    "if x0.shape != x3.shape:\n",
    "    print(\"WARNING: shape mismatch between inputs for different lead times (traindata)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _, _, x3, _, _= *test_pipeline(0), *test_pipeline(3)\n",
    "if x0.shape != x3.shape:\n",
    "    print(\"WARNING: shape mismatch between inputs for different lead times (testdata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data set\n",
    "\n",
    "For the training and testing of machine learning models it is crucial to split the data set into:\n",
    "\n",
    "1. __Train data set__ which is used to train the weights of the neural network\n",
    "\n",
    "2. __Validation data set__ which is used to check for overfitting (e.g. when using early stopping) and to optimize the hyperparameters \n",
    "\n",
    "3. __Test data set__ which is used to to evaluate the trained model. \n",
    "\n",
    "__NOTE:__ It is important to understand that hyperparamters must be tuned so that the result is best for the Validation data set and __not__ for the test data set. Otherwise you can not rule out the case that the specific hyperparameter setting just works good for the specific test data set but is not generally a good hyperparameter setting.\n",
    "\n",
    "In the following cell the train and the validation data set are still one data set, because this array will be later splitted into two arrays when th model is fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# from ninolearn.learn.models.dem import DEM\n",
    "\n",
    "# # clear memory from previous sessions\n",
    "# K.clear_session()\n",
    "\n",
    "# # define the lead time\n",
    "# lead_time = leadtime\n",
    "\n",
    "# # get the features (X), the label (y) and \n",
    "# # the time axis of the label (timey)\n",
    "# X, y, timey = pipeline(lead_time)\n",
    "\n",
    "# # split the data set into \n",
    "# # test_indeces = (timey>='1987-01-01') & (timey<='1993-12-01')\n",
    "# test_indeces = (timey>=t_end - pd.Timedelta(5*365, 'D')) & (timey<=t_end)\n",
    "\n",
    "# train_val_indeces = np.invert(test_indeces)\n",
    "\n",
    "# train_val_X, train_val_y, train_val_timey = X[train_val_indeces,:], y[train_val_indeces], timey[train_val_indeces]\n",
    "# testX, testy, testtimey = X[test_indeces,:], y[test_indeces], timey[test_indeces]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.shape, X.shape, timey.shape)\n",
    "# print('shapes of the data, labels and time axis is predictable, since there are now 4 features and 12 lags \\\n",
    "#     making for 48 columns. The labels are offset from the input data by the lead time ')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Now it is time to train the model! For this a random search is used for all keyword arguments that are passed in a *list* to the DEM.set_parameters() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initiated an instance of the DEM (Deep Ensemble Model) class\n",
    "# model = DEM()\n",
    "\n",
    "# # Set parameters\n",
    "# model.set_hyperparameters(searchtype='linear', layers=1, neurons=16, dropout=[0.1, 0.5], noise_in=[0.1,0.5], noise_sigma=[0.1,0.5],\n",
    "#                      noise_mu=[0.1,0.5], l1_hidden=[0.0, 0.2], l2_hidden=[0., 0.2],\n",
    "#                      l1_mu=[0.0, 0.2], l2_mu=[0.0, 0.2], l1_sigma=[0.0, 0.2],\n",
    "#                      l2_sigma=[0.0, 0.2], lr=[0.0001,0.01], batch_size=100, epochs=500, n_segments=5,\n",
    "#                      n_members_segment=1, patience=30, verbose=0, pdf='normal', activation = 'relu')\n",
    "\n",
    "# # Use a random search to find the optimal hyperparameters\n",
    "\n",
    "# model.fit_RandomizedSearch(train_val_X, train_val_y, train_val_timey, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_training(DEM, train_pipeline, n_iter = 1 , modelname = name, layers=1, neurons=16, dropout=[0.1, 0.5], noise_in=[0.1,0.5], noise_sigma=[0.1,0.5],\n",
    "                     noise_mu=[0.1,0.5], l1_hidden=[0.0, 0.2], l2_hidden=[0., 0.2],\n",
    "                     l1_mu=[0.0, 0.2], l2_mu=[0.0, 0.2], l1_sigma=[0.0, 0.2],\n",
    "                     l2_sigma=[0.0, 0.2], lr=[0.0001,0.01], batch_size=10, epochs = 50, n_segments = 5,\n",
    "                    n_members_segment =1, patience=5, verbose = 0, pdf='normal', activation='relu')\n",
    "# cross_training(DEM, pipeline, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_hindcast_dem(DEM, test_pipeline, name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = train_version + '_' + test_version\n",
    "r, p  = evaluation_decadal_correlation_ZC(name, variable_name='mean', ZC_version=test_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from ninolearn.private import plotdir\n",
    "plot_seasonal_skill_ZC(lead_times, r,  vmin=-1, vmax=1)\n",
    "# plt.contour(np.arange(1,5),lead_times, p, [0.9, 0.95, 0.99], linestyles=['solid', 'dashed', 'dotted'], colors='k')\n",
    "plt.title('Correlation skill')\n",
    "# plt.tight_layout()\n",
    "plt.savefig(join(plotdir, 'TL_r_skill_' + train_version + '_' + test_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions for the test data set\n",
    "Now we can use the trained models to make predicitons on the test data set to evaluate how good the model perfoms on a data set that it never saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_std = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ninolearn.learn.fit import cross_hindcast_dem, cross_hindcast\n",
    "# cross_hindcast(model, pipeline, 'DEM')\n",
    "# # cross_hindcast_dem(model, pipeline, 'DEM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the prediction\n",
    "Let's see how the predicion is looking like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ninolearn.plot.prediction import plot_prediction\n",
    "import pandas as pd\n",
    "from ninolearn.pathes import plotdir\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(15,3.5))\n",
    "plt.axhspan(-0.5,\n",
    "            -6,\n",
    "            facecolor='blue',\n",
    "            alpha=0.1,zorder=0)\n",
    "\n",
    "plt.axhspan(0.5,\n",
    "            6,\n",
    "            facecolor='red',\n",
    "            alpha=0.1,zorder=0)\n",
    "\n",
    "plt.xlim(testtimey[0], testtimey[-1])\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "# plot the prediction\n",
    "plot_prediction(testtimey, pred_mean, std=pred_std, facecolor='royalblue', line_color='navy')\n",
    "\n",
    "# plot the observation\n",
    "plt.plot(timey, y, \"r\", label = 'observation')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(join(plotdir, f'predicVSobs_{version}_{lead_time}lead'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "We can evaluate the model a bit more quantitatively using the loss function that was used to train the model, namely the negative-log-likelihood of the Gaussian and the correlation between the predicted mean and the observed ONI index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ninolearn.plot.evaluation import plot_correlation, plot_confMat, plot_seasonal_skill\n",
    "\n",
    "# loss = model.evaluate(testy, pred_mean, pred_std)\n",
    "# print(f\"Loss (Negative-Log-Likelihood): {loss}\")\n",
    "\n",
    "# # make a plot of the seasonal correaltion\n",
    "# # note: - pd.tseries.offsets.MonthBegin(1) appears to ensure that the correlations are plotted\n",
    "# # agains the correct season\n",
    "# plot_correlation(testy, pred_mean, testtimey - pd.tseries.offsets.MonthBegin(1), title=\"\")\n",
    "\n",
    "# # plot_seasonal_skill(leadtime, r_seas)\n",
    "\n",
    "# # plt.savefig(join(plotdir, f'correlation_{version}_{lead_time}lead'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seasonal_skill_ZC(lead_times, r,  vmin=0, vmax=1)\n",
    "plt.contour(np.arange(1,5),lead_times, p, linestyles=['solid', 'dashed', 'dotted'], colors='k')\n",
    "plt.title('Correlation skill')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
