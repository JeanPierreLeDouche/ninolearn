{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep ensemble for ENSO-forecasting\n",
    "\n",
    "In this tutorial you learn how to use a neural network model called Deep Ensemble (DE) for the ENSO forecasting. This network architecture was initially developed [Lakshminarayanan et al. (2017)](https://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf). \n",
    "\n",
    "DEs are feed foreword neural networks that predict the mean and the standard deviation of a Gaussian. Hence, their predicion comes with an uncertainty estimation which is a valuable feature for ENSO-forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data pipe line\n",
    "\n",
    "At first, we define a data pipeline. This is in general quite useful to keep your code clean and also to reuse the pipeline for later purpose.\n",
    "\n",
    "The data pipeline generates returns:\n",
    "\n",
    "1. The feature array\n",
    "\n",
    "2. The label array\n",
    "\n",
    "3. The time  array corresponding to the time of the label\n",
    "\n",
    "NOTE (again): Lead time is defined as the time that passed between the last observed and the first date of the target season. Hence, negative appear, e.g. if you compare the DJF season with the target season JFM, you have a lead time of -2 month (Last observed date: Feburary 28/29, First date of the target season January 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ninolearn.IO.read_processed import data_reader\n",
    "from ninolearn.IO.read_raw import ZC_simple_read\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ninolearn.learn.fit import n_decades, lead_times, decade_color, decade_name\n",
    "from ninolearn.learn.evaluation import evaluation_correlation, evaluation_decadal_correlation, evaluation_seasonal_correlation, evaluation_decadal_correlation_ZC\n",
    "from ninolearn.learn.fit import cross_hindcast_dem\n",
    "from ninolearn.plot.evaluation import plot_seasonal_skill_ZC\n",
    "import matplotlib.pyplot as plt\n",
    "from ninolearn.learn.fit import cross_training\n",
    "from ninolearn.learn.fit import cross_hindcast_dem\n",
    "from ninolearn.learn.models import DEM\n",
    "\n",
    "oneyear= pd.Timedelta(365, 'D')\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tstart = 1952-04-07 14:17:30 and tend = 1994-08-26 03:33:20 (train)\n",
      "tstart = 1952-04-07 14:17:30 and tend = 1994-08-26 03:33:20 (test)\n"
     ]
    }
   ],
   "source": [
    "train_version = 'mu28v4'\n",
    "# test_version = 'dt11v4'\n",
    "test_version = 'dt07v4'\n",
    "# leadtime = 12\n",
    "\n",
    "# t_start is defined using a funky timedelta because the starting date of the network analysis data is the last month\n",
    "# of its start year which is 1951-12 therefore the time must start in 1952 with some months added for values lost in \n",
    "# interpolation. TODO: fix this by backwards interpolating the first values of the year and finding out what is happening \n",
    "# with the nms\n",
    "train_times = np.unique(ZC_simple_read(train_version)['time'])\n",
    "test_times = np.unique(ZC_simple_read(test_version)['time'])\n",
    "\n",
    "train_t_start = train_times[0] + pd.Timedelta((2*365 + 90),'D')\n",
    "train_t_end = train_times[-1] - pd.Timedelta(90,'D')\n",
    "\n",
    "test_t_start = test_times[0] + pd.Timedelta((2*365 + 90),'D')\n",
    "test_t_end = test_times[-1] - pd.Timedelta(90,'D')\n",
    "\n",
    "print(f'tstart = {train_t_start} and tend = {train_t_end} (train)')\n",
    "print(f'tstart = {test_t_start} and tend = {test_t_end} (test)')\n",
    "\n",
    "t_start = train_t_start\n",
    "t_end = train_t_end\n",
    "times = train_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using dimensions ('lat', 'lon') from data variable temperature as the horizontal dimensions for this dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivo/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xesmf/frontend.py:524: FutureWarning: ``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.\n",
      "  keep_attrs=keep_attrs\n",
      "/home/ivo/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xesmf/smm.py:93: UserWarning: Input array is not C_CONTIGUOUS. Will affect performance.\n",
      "  warnings.warn(\"Input array is not C_CONTIGUOUS. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read sst climatetology\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: b'/home/ivo/Documents/GitHub/ninolearn/research/ivo_thesis/data/processed/sst_ZC_mu28v4.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/ivo/Documents/GitHub/ninolearn/research/ivo_thesis/data/processed/sst_ZC_mu28v4.nc',), 'a', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1e667d09b4ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## read raw ZC data and save to 1x1 grid file in processeddir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## also makes field of h and sst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mZC_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## calculates monthly averaged (?) fields of thermocline height within region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ninolearn/ninolearn/IO/read_raw.py\u001b[0m in \u001b[0;36mZC_raw\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0msst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anomaly'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msst_anom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0msst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesseddir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'sst_ZC_{version}.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mZC_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0munlimited_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlimited_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m             \u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minvalid_netcdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m         )\n\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0;34m\"unrecognized option 'invalid_netcdf' for engine %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             )\n\u001b[0;32m-> 1064\u001b[0;31m     \u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munlimited_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mnetCDF4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nc4_require_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;34m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_with_cache_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ninolearn/lib/python3.6/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;31m# ensure file doesn't get overriden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: b'/home/ivo/Documents/GitHub/ninolearn/research/ivo_thesis/data/processed/sst_ZC_mu28v4.nc'"
     ]
    }
   ],
   "source": [
    "from ninolearn.IO.read_raw import ZC_raw, ZC_h, ZC_oni\n",
    "from ninolearn.preprocess.prepare import prep_nms\n",
    "from ninolearn.plot.ZC_dem_plots import nms_plots\n",
    "## read raw ZC data and save to 1x1 grid file in processeddir\n",
    "## also makes field of h and sst\n",
    "ZC_raw(train_version)\n",
    "\n",
    "## calculates monthly averaged (?) fields of thermocline height within region \n",
    "## of interest. cacluate ONI in region of interest. calculate network metrics \n",
    "## from sst (Henk's suggestion) or thermocline height (like Paul)\n",
    "ZC_h(train_version)\n",
    "ZC_oni(train_version)\n",
    "\n",
    "prep_nms(train_version, 0.99, t_start, t_end)\n",
    "\n",
    "# make plots\n",
    "# nms_plots(train_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZC_raw(test_version)\n",
    "ZC_h(test_version)\n",
    "ZC_oni(test_version)\n",
    "prep_nms(test_version, 0.99, t_start, t_end)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reader = data_reader(startdate=(t_start + pd.Timedelta(365,'D')) , enddate=(t_end - pd.Timedelta(2*365, 'D')) , lon_min = 124, lon_max = 280,\n",
    "                         lat_min = -19, lat_max = 19)\n",
    "\n",
    "oni = reader.read_csv(('oni_ZC_' +train_version))\n",
    "h = reader.read_csv(('h_mean_ZC_' + train_version))\n",
    "\n",
    "network_ssh = reader.read_statistic('network_metrics', variable='sst', dataset=('ZC_25x25_' + train_version), processed=\"anom\")\n",
    "c2 = network_ssh['fraction_clusters_size_2']\n",
    "H = network_ssh['corrected_hamming_distance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if h.shape[0] + c2.shape[0] - H.shape[0] - oni.shape[0] == 0:\n",
    "    print('All datasets are of equal length')\n",
    "else:\n",
    "    print('warning: datasets not of equal size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ninolearn.utils import include_time_lag\n",
    "from ninolearn.IO.read_processed import data_reader\n",
    "\n",
    "version = train_version\n",
    "\n",
    "def train_pipeline(lead_time):\n",
    "    \"\"\"\n",
    "    Data pipeline for the processing of the data before the Deep Ensemble\n",
    "    is trained.\n",
    "\n",
    "    :type lead_time: int\n",
    "    :param lead_time: The lead time in month.\n",
    "\n",
    "    :returns: The feature \"X\" (at observation time), the label \"y\" (at lead\n",
    "    time), the target season \"timey\" (least month)\n",
    "    \"\"\"\n",
    "    timelag=False\n",
    "#     reader = data_reader(startdate='1952-01', enddate='1992-12', lon_min = 124, lon_max = 280,\n",
    "#                          lat_min = -19, lat_max = 19)\n",
    "    reader = data_reader(startdate=(t_start + oneyear), enddate=(t_end - 2*oneyear), lon_min = 124, lon_max = 280,\n",
    "                         lat_min = -19, lat_max = 19)\n",
    "\n",
    "    # indeces\n",
    "    oni = reader.read_csv(('oni_ZC_' +version))\n",
    "    h = reader.read_csv(('h_mean_ZC_' + version))\n",
    "    #IOD unavailable in ZC87 model \n",
    "    \n",
    "    # seasonal cycle\n",
    "    sc = np.cos(np.arange(len(oni))/12*2*np.pi)\n",
    "\n",
    "    # network metrics\n",
    "    network_ssh = reader.read_statistic('network_metrics', variable='sst', dataset=('ZC_25x25_'+version), processed=\"anom\")\n",
    "    c2 = network_ssh['fraction_clusters_size_2']\n",
    "    H = network_ssh['corrected_hamming_distance']\n",
    "\n",
    "    # time lag\n",
    "    time_lag = 12\n",
    "\n",
    "    # shift such that lead time corresponds to the definition of lead time\n",
    "    shift = 3\n",
    "\n",
    "    # process features\n",
    "    feature_unscaled = np.stack((oni, h,\n",
    "                                 c2, H), axis=1)\n",
    "\n",
    "    # scale each feature\n",
    "    scalerX = StandardScaler()\n",
    "    Xorg = scalerX.fit_transform(feature_unscaled)\n",
    "\n",
    "    # set nans to 0.\n",
    "    Xorg = np.nan_to_num(Xorg)\n",
    "\n",
    "    # arange the feature array\n",
    "    X = Xorg[:-lead_time-shift,:] # this chops of a bit at the end because matching labels will be offset by \n",
    "    # this amount. e.g. if our data runs until 2012 we need to remove X values for 2012 because we will use december 2011\n",
    "    # to predict december 2012 \n",
    "    \n",
    "#     X = include_time_lag(X, max_lag=time_lag)\n",
    "    X = include_time_lag(X, n_lags =time_lag)  # staggers the data with 1 month shifts so at each moment of input also\n",
    "    # nlags amount of months before is available to the AI\n",
    "        \n",
    "    # arange label\n",
    "    yorg = oni.values\n",
    "    y = yorg[lead_time + time_lag + shift:] # labels offset by lead_time to predict into the future and time_lag \n",
    "    # because the include_time_lag function shifts X values forward by an amount n_lags=time_lag\n",
    "    \n",
    "    # get the time axis of the label\n",
    "    timey = oni.index[lead_time + time_lag + shift:]\n",
    "\n",
    "    if timelag == False:\n",
    "        X = Xorg\n",
    "        y = yorg\n",
    "        timey = oni.index\n",
    "        \n",
    "    return X, y, timey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = test_version\n",
    "\n",
    "def test_pipeline(lead_time):\n",
    "    \"\"\"\n",
    "    Data pipeline for the processing of the data before the Deep Ensemble\n",
    "    is trained.\n",
    "\n",
    "    :type lead_time: int\n",
    "    :param lead_time: The lead time in month.\n",
    "\n",
    "    :returns: The feature \"X\" (at observation time), the label \"y\" (at lead\n",
    "    time), the target season \"timey\" (least month)\n",
    "    \"\"\"\n",
    "    timelag=False\n",
    "#     reader = data_reader(startdate='1952-01', enddate='1992-12', lon_min = 124, lon_max = 280,\n",
    "#                          lat_min = -19, lat_max = 19)\n",
    "    reader = data_reader(startdate=(t_start + oneyear), enddate=(t_end - 2*oneyear), lon_min = 124, lon_max = 280,\n",
    "                         lat_min = -19, lat_max = 19)\n",
    "\n",
    "    # indeces\n",
    "    oni = reader.read_csv(('oni_ZC_' +version))\n",
    "    h = reader.read_csv(('h_mean_ZC_' + version))\n",
    "    #IOD unavailable in ZC87 model \n",
    "    \n",
    "    # seasonal cycle\n",
    "    sc = np.cos(np.arange(len(oni))/12*2*np.pi)\n",
    "\n",
    "    # network metrics\n",
    "    network_ssh = reader.read_statistic('network_metrics', variable='sst', dataset=('ZC_25x25_'+version), processed=\"anom\")\n",
    "    c2 = network_ssh['fraction_clusters_size_2']\n",
    "    H = network_ssh['corrected_hamming_distance']\n",
    "\n",
    "    # time lag\n",
    "    time_lag = 12\n",
    "\n",
    "    # shift such that lead time corresponds to the definition of lead time\n",
    "    shift = 3\n",
    "\n",
    "    # process features\n",
    "    feature_unscaled = np.stack((oni, h,\n",
    "                                 c2, H), axis=1)\n",
    "\n",
    "    # scale each feature\n",
    "    scalerX = StandardScaler()\n",
    "    Xorg = scalerX.fit_transform(feature_unscaled)\n",
    "\n",
    "    # set nans to 0.\n",
    "    Xorg = np.nan_to_num(Xorg)\n",
    "\n",
    "    # arange the feature array\n",
    "    X = Xorg[:-lead_time-shift,:] # this chops of a bit at the end because matching labels will be offset by \n",
    "    # this amount. e.g. if our data runs until 2012 we need to remove X values for 2012 because we will use december 2011\n",
    "    # to predict december 2012 \n",
    "    \n",
    "#     X = include_time_lag(X, max_lag=time_lag)\n",
    "    X = include_time_lag(X, n_lags =time_lag)  # staggers the data with 1 month shifts so at each moment of input also\n",
    "    # nlags amount of months before is available to the AI\n",
    "        \n",
    "    # arange label\n",
    "    yorg = oni.values\n",
    "    y = yorg[lead_time + time_lag + shift:] # labels offset by lead_time to predict into the future and time_lag \n",
    "    # because the include_time_lag function shifts X values forward by an amount n_lags=time_lag\n",
    "    \n",
    "    # get the time axis of the label\n",
    "    timey = oni.index[lead_time + time_lag + shift:]\n",
    "\n",
    "    if timelag == False:\n",
    "        X = Xorg\n",
    "        y = yorg\n",
    "        timey = oni.index\n",
    "        \n",
    "    return X, y, timey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _, _, x3, _, _= *train_pipeline(0), *train_pipeline(3)\n",
    "if x0.shape != x3.shape:\n",
    "    print(\"WARNING: shape mismatch between inputs for different lead times (traindata)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, _, _, x3, _, _= *test_pipeline(0), *test_pipeline(3)\n",
    "if x0.shape != x3.shape:\n",
    "    print(\"WARNING: shape mismatch between inputs for different lead times (testdata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data set\n",
    "\n",
    "For the training and testing of machine learning models it is crucial to split the data set into:\n",
    "\n",
    "1. __Train data set__ which is used to train the weights of the neural network\n",
    "\n",
    "2. __Validation data set__ which is used to check for overfitting (e.g. when using early stopping) and to optimize the hyperparameters \n",
    "\n",
    "3. __Test data set__ which is used to to evaluate the trained model. \n",
    "\n",
    "__NOTE:__ It is important to understand that hyperparamters must be tuned so that the result is best for the Validation data set and __not__ for the test data set. Otherwise you can not rule out the case that the specific hyperparameter setting just works good for the specific test data set but is not generally a good hyperparameter setting.\n",
    "\n",
    "In the following cell the train and the validation data set are still one data set, because this array will be later splitted into two arrays when th model is fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "# from ninolearn.learn.models.dem import DEM\n",
    "\n",
    "# # clear memory from previous sessions\n",
    "# K.clear_session()\n",
    "\n",
    "# # define the lead time\n",
    "# lead_time = leadtime\n",
    "\n",
    "# # get the features (X), the label (y) and \n",
    "# # the time axis of the label (timey)\n",
    "# X, y, timey = pipeline(lead_time)\n",
    "\n",
    "# # split the data set into \n",
    "# # test_indeces = (timey>='1987-01-01') & (timey<='1993-12-01')\n",
    "# test_indeces = (timey>=t_end - pd.Timedelta(5*365, 'D')) & (timey<=t_end)\n",
    "\n",
    "# train_val_indeces = np.invert(test_indeces)\n",
    "\n",
    "# train_val_X, train_val_y, train_val_timey = X[train_val_indeces,:], y[train_val_indeces], timey[train_val_indeces]\n",
    "# testX, testy, testtimey = X[test_indeces,:], y[test_indeces], timey[test_indeces]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.shape, X.shape, timey.shape)\n",
    "# print('shapes of the data, labels and time axis is predictable, since there are now 4 features and 12 lags \\\n",
    "#     making for 48 columns. The labels are offset from the input data by the lead time ')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Now it is time to train the model! For this a random search is used for all keyword arguments that are passed in a *list* to the DEM.set_parameters() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initiated an instance of the DEM (Deep Ensemble Model) class\n",
    "# model = DEM()\n",
    "\n",
    "# # Set parameters\n",
    "# model.set_hyperparameters(searchtype='linear', layers=1, neurons=16, dropout=[0.1, 0.5], noise_in=[0.1,0.5], noise_sigma=[0.1,0.5],\n",
    "#                      noise_mu=[0.1,0.5], l1_hidden=[0.0, 0.2], l2_hidden=[0., 0.2],\n",
    "#                      l1_mu=[0.0, 0.2], l2_mu=[0.0, 0.2], l1_sigma=[0.0, 0.2],\n",
    "#                      l2_sigma=[0.0, 0.2], lr=[0.0001,0.01], batch_size=100, epochs=500, n_segments=5,\n",
    "#                      n_members_segment=1, patience=30, verbose=0, pdf='normal', activation = 'relu')\n",
    "\n",
    "# # Use a random search to find the optimal hyperparameters\n",
    "\n",
    "# model.fit_RandomizedSearch(train_val_X, train_val_y, train_val_timey, n_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_training(DEM, train_pipeline, n_iter = 1 , layers=1, neurons=16, dropout=[0.1, 0.5], noise_in=[0.1,0.5], noise_sigma=[0.1,0.5],\n",
    "                     noise_mu=[0.1,0.5], l1_hidden=[0.0, 0.2], l2_hidden=[0., 0.2],\n",
    "                     l1_mu=[0.0, 0.2], l2_mu=[0.0, 0.2], l1_sigma=[0.0, 0.2],\n",
    "                     l2_sigma=[0.0, 0.2], lr=[0.0001,0.01], batch_size=10, epochs = 50, n_segments = 5,\n",
    "                    n_members_segment =1, patience=5, verbose = 0, pdf='normal', activation='relu')\n",
    "# cross_training(DEM, pipeline, n_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_hindcast_dem(DEM, test_pipeline, 'dem') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = train_version + '_' + test_version\n",
    "r, p  = evaluation_decadal_correlation_ZC('dem', variable_name='mean', ZC_version=test_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from ninolearn.private import plotdir\n",
    "plot_seasonal_skill_ZC(lead_times, r,  vmin=0, vmax=1)\n",
    "# plt.contour(np.arange(1,5),lead_times, p, [0.9, 0.95, 0.99], linestyles=['solid', 'dashed', 'dotted'], colors='k')\n",
    "plt.title('Correlation skill')\n",
    "# plt.tight_layout()\n",
    "plt.savefig(join(plotdir, 'TL_r_skill_' + train_version + '_' + test_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions for the test data set\n",
    "Now we can use the trained models to make predicitons on the test data set to evaluate how good the model perfoms on a data set that it never saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean, pred_std = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ninolearn.learn.fit import cross_hindcast_dem, cross_hindcast\n",
    "# cross_hindcast(model, pipeline, 'DEM')\n",
    "# # cross_hindcast_dem(model, pipeline, 'DEM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the prediction\n",
    "Let's see how the predicion is looking like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ninolearn.plot.prediction import plot_prediction\n",
    "import pandas as pd\n",
    "from ninolearn.pathes import plotdir\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(15,3.5))\n",
    "plt.axhspan(-0.5,\n",
    "            -6,\n",
    "            facecolor='blue',\n",
    "            alpha=0.1,zorder=0)\n",
    "\n",
    "plt.axhspan(0.5,\n",
    "            6,\n",
    "            facecolor='red',\n",
    "            alpha=0.1,zorder=0)\n",
    "\n",
    "plt.xlim(testtimey[0], testtimey[-1])\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "# plot the prediction\n",
    "plot_prediction(testtimey, pred_mean, std=pred_std, facecolor='royalblue', line_color='navy')\n",
    "\n",
    "# plot the observation\n",
    "plt.plot(timey, y, \"r\", label = 'observation')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(join(plotdir, f'predicVSobs_{version}_{lead_time}lead'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "We can evaluate the model a bit more quantitatively using the loss function that was used to train the model, namely the negative-log-likelihood of the Gaussian and the correlation between the predicted mean and the observed ONI index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ninolearn.plot.evaluation import plot_correlation, plot_confMat, plot_seasonal_skill\n",
    "\n",
    "# loss = model.evaluate(testy, pred_mean, pred_std)\n",
    "# print(f\"Loss (Negative-Log-Likelihood): {loss}\")\n",
    "\n",
    "# # make a plot of the seasonal correaltion\n",
    "# # note: - pd.tseries.offsets.MonthBegin(1) appears to ensure that the correlations are plotted\n",
    "# # agains the correct season\n",
    "# plot_correlation(testy, pred_mean, testtimey - pd.tseries.offsets.MonthBegin(1), title=\"\")\n",
    "\n",
    "# # plot_seasonal_skill(leadtime, r_seas)\n",
    "\n",
    "# # plt.savefig(join(plotdir, f'correlation_{version}_{lead_time}lead'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seasonal_skill_ZC(lead_times, r,  vmin=0, vmax=1)\n",
    "plt.contour(np.arange(1,5),lead_times, p, linestyles=['solid', 'dashed', 'dotted'], colors='k')\n",
    "plt.title('Correlation skill')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
