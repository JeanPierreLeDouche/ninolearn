{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardized Research\n",
    "\n",
    "This tutorial covers in a simple example of a Lasso regression model all steps in the development process of a new \n",
    "model that follows the standards of NinoLearn.\n",
    "\n",
    "## Download\n",
    "\n",
    "Download four indeces which we want to use to predict the Oceaninc Nino Index (ONI):\n",
    "\n",
    "1. The ONI index itself.\n",
    "2. The Dipole Mode Index of the Indian Ocean Dipole (IOD)\n",
    "3. The Warm Water Volume (WWV)\n",
    "4. The Kiritimati Index that can be used as WWV proxy form 1955 onwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oni.txt already downloaded\n",
      "iod.txt already downloaded\n",
      "wwv.dat already downloaded\n",
      "Copy Kindex.mat to data directory\n"
     ]
    }
   ],
   "source": [
    "from ninolearn.download import download, sources\n",
    "\n",
    "download(sources.ONI)\n",
    "download(sources.IOD)\n",
    "download(sources.WWV)\n",
    "download(sources.KINDEX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Extract the essential data form the raw files and move them into preprocessed data directory. If you are interested what these functions exactly do, check out there source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare ONI timeseries.\n",
      "Prepare WWV  timeseries.\n",
      "Prepare IOD timeseries.\n"
     ]
    }
   ],
   "source": [
    "from ninolearn.preprocess.prepare import prep_oni, prep_wwv\n",
    "from ninolearn.preprocess.prepare import prep_iod, prep_K_index, prep_wwv_proxy\n",
    "\n",
    "\n",
    "prep_oni()\n",
    "prep_wwv()\n",
    "prep_iod()\n",
    "prep_K_index()\n",
    "\n",
    "# combines the WWV and the K-Index to one WWV proxy\n",
    "\n",
    "prep_wwv_proxy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a new Model\n",
    "\n",
    "Here an example of an multilinear (Lasso) regression model is given that is based on the scikit-learn python package.\n",
    "\n",
    "### Data pipeline\n",
    "First a data pipeline is built. The pipeline is used during training, prediction and evaluation to generate the feature, the label, the time as well as (optional) the persistence forecast.\n",
    "\n",
    "When you build a new data pipeline, it needs to have the same structure as the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data reader to read data from the preprocessed data directory\n",
    "from ninolearn.IO.read_processed import data_reader\n",
    "import numpy as np\n",
    "\n",
    "def pipeline(lead_time,  return_persistance=False):\n",
    "    \"\"\"\n",
    "    Data pipeline for the processing of the data before the lasso regression model\n",
    "    is trained.\n",
    "\n",
    "    :type lead_time: int\n",
    "    :param lead_time: The lead time in month.\n",
    "\n",
    "    :type return_persistance: boolean\n",
    "    :param return_persistance: Return as the persistefnce as well.\n",
    "\n",
    "    :returns: The feature \"X\" (at observation time), the label \"y\" (at lead\n",
    "    time), the target season \"timey\" (least month) and if selected the\n",
    "    label at observation time \"y_persistance\". Hence, the output comes as:\n",
    "    X, y, timey, y_persistance.\n",
    "    \"\"\"\n",
    "    # initialize the reader\n",
    "    reader = data_reader(startdate='1960-01', enddate='2017-12')\n",
    "\n",
    "    # Load data \n",
    "    # HERE you could load other data sources\n",
    "    oni = reader.read_csv('oni')\n",
    "    wwv = reader.read_csv('wwv_proxy')\n",
    "    iod = reader.read_csv('iod')\n",
    "    \n",
    "    # the shift data by 3 in addition to lead time shift (due to definition\n",
    "    # of lead time) as in barnston et al. (2012)\n",
    "    shift = 3\n",
    "    \n",
    "    # Make feature\n",
    "    # HERE you need to stack you data if you loaded different data sets\n",
    "    Xorg = np.stack((oni, wwv, iod), axis=1)\n",
    "    X = Xorg[:-lead_time-shift,:]\n",
    "\n",
    "    # arange label\n",
    "    yorg = oni.values\n",
    "    y = yorg[lead_time + shift:]\n",
    "\n",
    "    # get the time axis of the label\n",
    "    timey = oni.index[lead_time + shift:]\n",
    "\n",
    "    if return_persistance:\n",
    "        y_persistance = yorg[: - lead_time - shift]\n",
    "        return X, y, timey, y_persistance\n",
    "    else:\n",
    "        return X, y, timey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "First of all, the newly designed model needs to inherit from the baseModel such that the model can be trained and evaluated in a standardized procedure furhter down.\n",
    "\n",
    "The model has mandatory variables and functions that need to be included. These parts are highlighted in the following with the comment \"MANDATORY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the baseModel from which the mlr class needs to inherit\n",
    "from ninolearn.learn.models.baseModel import baseModel\n",
    "\n",
    "# import the sklearn model that we want to use for the ENSO forecast\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# import some packages and methods to saving the model later\n",
    "import pickle\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "\n",
    "# MANDATORY: Needs to in herit from the class baseModel\n",
    "class mlr(baseModel):\n",
    "\n",
    "    # MANDETORY: Define how many outputs your model has\n",
    "    n_outputs=1\n",
    "    \n",
    "    # MANDETORY: The name that is used when predictions are saved in an netCDF file.\n",
    "    output_names = ['prediction']\n",
    "\n",
    "    \n",
    "    # MANDATORY: The model needs to have a .__init__() method.\n",
    "    def __init__(self, alpha=1.0, name='mlr'):\n",
    "        \"\"\"\n",
    "        The model needs to have an __init__ function. That takes contains\n",
    "        receives the hyperparameters of the model as well as the name of the\n",
    "        model as keyword arguments\n",
    "        \n",
    "        :type alpha: float\n",
    "        :param alpha: The coefficent for the lasso penatly term.\n",
    "        \n",
    "        :type name: str\n",
    "        :param name: The name of the model that is used to save it to a file after training.\n",
    "        \"\"\"\n",
    "        #  MANDATORY: Apply the .set_hyperparameters function to all keyword arguments.\n",
    "        self.set_hyperparameters(alpha=alpha, name=name)\n",
    "\n",
    "        \n",
    "        \n",
    "    # MANDATORY: The model needs to have a .fit() function that takes trainX, trainy as arguments. \n",
    "    # Very complex models, e.g. neural networks would need to split the trainX and trainy variables further\n",
    "    # to generate a validation data set, which is then used to calculate\n",
    "    # the self.mean_val_loss and to check for overfitting.\n",
    "    # Here, we don't need to do so because the model is not very complex and\n",
    "    # we have plenty of data to train the model.\n",
    "    def fit(self, trainX, trainy):\n",
    "        \"\"\"\n",
    "        This is the fit function of the model. \n",
    "        \n",
    "        :param trainX: The features.\n",
    "        :param trainy: The label.\n",
    "        \"\"\"\n",
    "        #Initialize the Lasso model form the sci-kit learn package\n",
    "        self.model = Lasso(self.hyperparameters['alpha'])\n",
    "\n",
    "        # fit the model to the training data\n",
    "        self.model.fit(trainX,trainy)\n",
    "\n",
    "        # MANDETORY: Save the Score under self.mean_val_loss. This variable\n",
    "        # will be used to be optimized during the random search later\n",
    "        self.mean_val_loss = self.model.score(trainX, trainy)\n",
    "    \n",
    "    # MANDATORY: The model needs to have a .fit() function that takes as arguments. \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make a prediction.\n",
    "        \n",
    "        :param: A feature set.\n",
    "        \"\"\"\n",
    "        # MANDATORY: Function needs to return a value (the prediction).\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    # MANDATORY: The model needs to have a .save() function. The location where to save the model \n",
    "    # is defined by the keyword arguments 'location' and 'name'\n",
    "    def save(self, location='', dir_name='mlr'):\n",
    "        \"\"\"\n",
    "        Arguments of this function are mandetory and used to systemically\n",
    "        save models in your modeldir.\n",
    "        \"\"\"\n",
    "        path = join(location, dir_name)\n",
    "        if not exists(path):\n",
    "            mkdir(path)\n",
    "        filename = join(path,f'model.sav')\n",
    "        pickle.dump(self.model, open(filename, 'wb'))\n",
    "    \n",
    "    # MANDATORY: The model needs to have a .load() function. The location where to saved model can be found \n",
    "    # is defined by the keyword arguments 'location' and 'name'\n",
    "    def load(self, location='', dir_name='mlr'):\n",
    "        \"\"\"\n",
    "        Arguments of this function are mandatory and used to systematically\n",
    "        load models from your modeldir.\n",
    "        \"\"\"\n",
    "        path = join(location, dir_name)\n",
    "        filename = join(path,f'model.sav')\n",
    "        self.model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross train the model\n",
    "\n",
    "In the cross_training() function the model is trained on a 5 of 6 time \"decades\" (1962-1971, 1972-1981,..., 2012-2018). For each decade, 50 search iterations with a random uniform choice of `alpha` between 0. and 0.001 is performed. The model that has the best score (in terms of `self.mean_val_loss`, see above) is saved in the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################################\n",
      "Lead time: 0 month\n",
      "##################################################################\n",
      "\n",
      "Test period: 1953-01-01 till 1961-12-01\n",
      "--------------------------------------\n",
      "Search iteration Nr 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "fit() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-455780c7a52b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mninolearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcross_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mlr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ninolearn/ninolearn/learn/fit.py\u001b[0m in \u001b[0;36mcross_training\u001b[0;34m(model, pipeline, n_iter, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraintime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indeces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indeces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indeces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_RandomizedSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraintime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodeldir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ninolearn/ninolearn/learn/models/baseModel.py\u001b[0m in \u001b[0;36mfit_RandomizedSearch\u001b[0;34m(self, trainX, trainy, timey, n_iter, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_hyp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: fit() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from ninolearn.learn.fit import cross_training\n",
    "cross_training(mlr, pipeline, 10, alpha=[0.,0.001],  name='mlr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make hindcast\n",
    "\n",
    "Now, each model makes the forecast for the decade on which it was NOT trained by the function cross_training()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ninolearn.learn.fit import cross_hindcast\n",
    "cross_hindcast(mlr, pipeline, 'mlr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally the model can be evaluated using the Pearson correlation and she standardized root-mean-squarred error (SRMSE). The SRMSE is the RMSE that is divided by the standard deviation of each season. This skill measure needs to be used instead of the RMSE  because the ONI has a seasonal cycle of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "from ninolearn.learn.fit import n_decades, lead_times, decade_color, decade_name\n",
    "from ninolearn.learn.evaluation import evaluation_correlation, evaluation_decadal_correlation, evaluation_seasonal_correlation\n",
    "from ninolearn.learn.evaluation import evaluation_srmse, evaluation_decadal_srmse, evaluation_seasonal_srmse\n",
    "from ninolearn.plot.evaluation import plot_seasonal_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# All season correlation skill\n",
    "# =============================================================================\n",
    "\n",
    "plt.close(\"all\")\n",
    "# scores on the full time series\n",
    "r, p  = evaluation_correlation('mlr', variable_name='prediction')\n",
    "\n",
    "# score in different decades\n",
    "r_dec, p_dec = evaluation_decadal_correlation('mlr', variable_name='prediction')\n",
    "\n",
    "# plot correlation skills\n",
    "ax = plt.figure(figsize=(6.5,3.5)).gca()\n",
    "\n",
    "for j in range(n_decades-1):\n",
    "    plt.plot(lead_times, r_dec[:,j], c=decade_color[j], label=f\"Deep Ens.  ({decade_name[j]})\")\n",
    "plt.plot(lead_times, r, label=\"Deep Ens.  (1962-2017)\", c='k', lw=2)\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0.,lead_times[-1])\n",
    "plt.xlabel('Lead Time [Months]')\n",
    "plt.ylabel('r')\n",
    "plt.grid()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# All season SRMSE skill\n",
    "# =============================================================================\n",
    "srmse_dec = evaluation_decadal_srmse('mlr', variable_name='prediction')\n",
    "srmse = evaluation_srmse('mlr', variable_name='prediction')\n",
    "\n",
    "# plot SRMSE skills\n",
    "ax = plt.figure(figsize=(6.5,3.5)).gca()\n",
    "for j in range(n_decades-1):\n",
    "    plt.plot(lead_times, srmse_dec[:,j], c=decade_color[j], label=f\"Deep Ens.  ({decade_name[j]})\")\n",
    "plt.plot(lead_times, srmse, label=\"Deep Ens.  (1962-2017)\", c='k', lw=2)\n",
    "\n",
    "plt.ylim(0,1.5)\n",
    "plt.xlim(0.,lead_times[-1])\n",
    "plt.xlabel('Lead Time [Months]')\n",
    "plt.ylabel('SRMSE')\n",
    "plt.grid()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Seasonal skills\n",
    "# =============================================================================\n",
    "# evaluate the model in different seasons\n",
    "r_seas, p_seas = evaluation_seasonal_correlation('mlr', variable_name='prediction')\n",
    "\n",
    "plot_seasonal_skill(lead_times, r_seas,  vmin=0, vmax=1)\n",
    "plt.contour(np.arange(1,13),lead_times, p_seas, levels=[0.01, 0.05, 0.1], linestyles=['solid', 'dashed', 'dotted'], colors='k')\n",
    "plt.title('Correlation skill')\n",
    "plt.tight_layout()\n",
    "\n",
    "srsme_seas = evaluation_seasonal_srmse('mlr', variable_name='prediction')\n",
    "plot_seasonal_skill(lead_times, srsme_seas,  vmin=0, vmax=1.5, cmap=plt.cm.inferno_r, extend='max')\n",
    "plt.title('SRMSE skill')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
